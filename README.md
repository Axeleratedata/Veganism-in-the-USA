### Veganism-in-the-USA

#### Objective:
This is a Capstone project on vegan and vegetarian restaurants in the USA. The goal of this project was to obtain the data through web scraping and then analyze the features of the restuarants. Once the data was wrangled and engineered ML models were created to classify the restuarants as either vegan or vegeterian.

#### Partner:
This project was completed as part of the Data Science Immersive Course at General Assembly in London.

#### Methods Used:
Selenium & Beautiful Soup
Inferential Statistics
Machine Learning
Data Visualization
Predictive Modeling

#### Technologies:
Jupyter
Python
Chromedriver
Pandas
scikit-learn
matplotlib
HTML

#### Description:
My capstone project focused on veganism in the USA. I found a vegan/vegetarian restaurant database that had an abundance of location-specific restaurant information, which was perfect for the data I was after. However the website was rigged with cyber security tools (like Incapsula) that made scraping a daunting task. I decided to narrow down the data requirements by filtering out all restaurants that are purely vegan and vegetarian. This removed almost all restaurant chains and restaurants that offered vegan/veggie options bringing the total number to around 3100. These restaurants were divided into unique city URL’s of 1000+. In this list of URLS, were randomly generated honey-pot URLs. These URLs were not in the sitemap or linked to from anywhere on the site, so if the bot landed on one, the IP address was immediately blocked on the guise of a ‘Down For Maintenance’ webpage. 

The IP address could also be blocked by not logging in when prompted by the pop up, the bot would load the new page and the pop up would reappear. After so many reloads the maintenance page reapers. Similarly if too many requests were sent in quick succession a reCAPTCHA page appeared that needed to be completed manually before continuing. I was able to overcome all these issues but the last. The rotating IP function would fetch working IPs, the selenium bot logged in automatically whenever the pop up appeared, and the honeypot URLs were filtered out by importing a csv file of all cities and towns in the USA to cross reference it against. However the reCAPTHCA had to be left to the human and done manually.

Scraping the data was a lengthy and difficult task. However I managed to get about 80% of the restaurants, with a clear idea of how to get 100% in the future. Once the data was scraped, it was combined, explored and cleaned. This was the second most time consuming stage, but one I also really enjoyed. Every column but the restaurant name had issues that needed to be engineered. The data was accurate however the website relies on user generated content so the inputs were not completely uniform. This created problems when trying to cleanly split the address column for example, but nothing a little RexEx couldn't solve. Similarly the cuisine column was user generated and there ended up being a lot of discrepancies in spelling and punctuation of the same cuisine. 

Once the data had been engineered I began visualizing what I had left. I started off with a waffle chart to display the split between the vegan (54%) and vegetarian (46%) restaurants. I then created a set of word clouds based off the cuisine column and split them up by states. There was however no discernible difference in the cuisine types of the states that had enough restaurants to create a worthy word cloud. I masked the word clouds onto images of the state or images that i thought represented the state (crocodile = Florida, truck = Texas etc.) Lastly I created choropleth maps of the US states and the occurrence of vegan and veggie restaurants within them. I wanted to make the maps relative to state population size. But that ended up making the maps somewhat skewed as some smaller states had a relatively large number of vegan/veggie restaurants. So like any good data scientist I manipulated the data! Once I removed Hawaii and Rhode island the map was a much illustration of the saturation of restaurants across America. I then compared my choropleth maps to ones generated by Google Trends. Astonishingly there was an almost identical relationship between search popularity for ‘Veganism’ and ‘Vegetarianism’ and the number of vegan/veggie restaurants in those states. 

The final etape was the one I was dreading the most, the modelling. I've never had trouble with mathematics, but it's been such a long time since I dealt with this level of maths so I've been on a mathematical back foot since the start of the course, however I was able to create a couple of good machine learning models.. The problem ended up being a classification one - predicting whether a restaurant falls into the vegan or veggie class based on its features. I built a Logistic Regression, Decision Tree, SVM, and finally a Random Forest using Bagging. At first they were all ‘perfect’ as words like vegan and vegetarian in the cuisine gave the class away. I looked at a correlation table and a feature importance table to see what cuisines to remove to make for a fairer model. In the end words like ‘vegan’, ‘vegetarian’, ‘raw’, ‘lacto’ were removed which made my scores more realistic. 

#### Getting Started
Clone this repo 

Run the code
